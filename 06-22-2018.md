## Task Today ##
- [x] Re-play [SQL](https://courses.edx.org/courses/course-v1:Microsoft+DAT201x+1T2018a/progress)
  - Lab 9,10
- [x] Reviewing ISYE6501 HW2  

```
**********************************************************************************
*********************       K-means Clustering    ********************************
**********************************************************************************

# table() uses the cross-classifying factors 
# to build a contingency table of the counts at each combination of factor levels.
table(data[,5], data$Species)

# -------------------------- Visualizing Data -------------------------------------
# library ggplot2 provides a powerful model of graphics 
# that makes it easy to produce complex multi-layered graphics.
library(ggplot2)
ggplot(data, aes(Petal.Length, Petal.Width, color=Species)) + geom_point()
ggplot(data, aes(Sepal.Length, Sepal.Width, color=Species)) + geom_point()

# -------------------------- Performing the kmeans clustering ---------------------
# using all attributes with k=3, 20 different random starting assignments and
# select the one with the lowest within cluster variation
irisClusterAll3 <- kmeans(data[,1:4], 3, nstart=20)

# This is how we'd get Lloyd's algorithm:
# irisClusterALL2lloyd <- kmeans(iris[,1:4], 2, algorithm="Lloyd", nstart = 20)

# scale data !!!!!!!
scdata <- data
for (i in 1:4) { scdata[,i] <- (data[,i]-min(data[,i])) / (max(data[,i])-min(data[,i])) }
irisClusterscAll3 <- kmeans(scdata[,1:4], 3, nstart=20)

# initial distance to zero
csum = 0

# for each data point
for (i in 1:nrow(data)) {
    # add distance between its point and its cluster center
    csum = csum + dist(rbind(data[i,1:4], irsiClusterAll3$centers[irisClusterAll3$cluster[i],]))
}

table(irisClusterALL3$cluster, data$Species)
table(irisClusterALLsc3$cluster, data$Species)
```

```
**********************************************************************************
***********************    Check the outlier    **********************************
**********************************************************************************

# Run the Shapiro-Wilk test to test the normality of the crime data
shapiro.test(crime)
# if p-value<0.1, we can reject the null hypothesis that data is normally distributed
# But they may focus too much on a few data point
# That's especially true if there are outliers in the data

# Look at the Q-Q plot of the crime data as another method to test the normality of the data
qqnorm(crime)
qqnorm(scale(crime))

test <- grubbs.test(crime, type = 10)
# Depending on our threshold p-value, we might or might not choose to call than an outlier.  
# For example, some people use p=0.05 as a threshold, some use p=0.10.
# With p-value = 1, at least one of the extremes (highest or lowest) is NOT an outlier.


# -------------------------- Outlier Visualization -------------------------------------
install.packages("ggplot2")
library(ggplot2)

# Define a function that finds 5%, 25%, 50%, 75%, and 95% quantiles of the data
quant <- function(x) {
  r <- quantile(x, probs = c(0.05, 0.25, 0.5, 0.75, 0.95))
  names(r) <- c("ymin", "lower", "middle", "upper", "ymax")
  r
}

# Create data frame with just the crime data
df <- data.frame(x = rep(1, nrow(data)), y = crime)

# Define a function that finds points below and aboce the 5% and 95% quantiles of the data
outliers <- function(x) {
  subset(x, x < quantile(x, 0.05) | quantile(x, 0.95) < x)
}

# Create the box-and-whisker plot
ggplot(df, aes(x, y)) + 
  stat_summary(fun.data = quant, geom="boxplot") + 
  stat_summary(fun.y = outliers, geom="point")
# This visualization shows us that the two cities with the highest amount of crime seem to be outliers.
# The two cities with the lowest amount of crime are close enough to not necessarily be considered outliers.
```
